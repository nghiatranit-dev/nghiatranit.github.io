---
---

@inproceedings{lu2024consistency,
  title={Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models},
  author={Cheng Lu and Yang Song},
  abbr={ICLR},
  abstract={Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.},
  pdf={https://arxiv.org/abs/2410.11081},
  year={2025},  
  honor={Oral Presentation [Top 1.8%]},
  award={Oral},
  poster={ICLR2025/scm_poster.pdf},
  blog={https://openai.com/index/simplifying-stabilizing-and-scaling-continuous-time-consistency-models/},
  booktitle={the 13th International Conference on Learning Representations, 2025.},
  selected={true}
}


@inproceedings{zhang2024deps,
  title={Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing},
  author={Bingliang Zhang and Wenda Chu and Julius Berner and Chenlin Meng and Anima Anandkumar and Yang Song},
  abbr={CVPR},
  abstract={Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems. For example, we achieve a PSNR of 30.72dB on the FFHQ 256 dataset for phase retrieval, which is an improvement of 9.12dB compared to existing methods.},
  pdf={https://arxiv.org/abs/2407.01521},
  award={Oral},
  code={https://github.com/zhangbingliang2019/DAPS},
  honor={Oral Presentation [Top 3.3%]},
  website={https://daps-inverse-problem.github.io/},
  booktitle={the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025.},
  year={2025},    
}

@inproceedings{song2023consistency2,
  title={Improved Techniques for Training Consistency Models},
  author={Yang Song and Prafulla Dhariwal},
  abbr={ICLR},
  abstract={Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64×64 respectively in a single sampling step. These scores mark a 3.5× and 4× improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.},
  pdf={https://arxiv.org/abs/2310.14189},
  poster={ICLR2024/ict.pdf},
  year={2024},
  award={Oral},
	honor={Oral Presentation [Top 1.2%]},
  booktitle={the 12th International Conference on Learning Representations, 2024.},
  selected={true}
}

@inproceedings{dou2024diffusion,
  title={Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective},
  author={Zehao Dou and Yang Song},
  abbr={ICLR},
  abstract={Diffusion models have achieved tremendous success in generating high-dimensional data like images, videos and audio. These models provide powerful data priors that can solve linear inverse problems in zero shot through Bayesian posterior sampling. However, exact posterior sampling for diffusion models is intractable. Current solutions often hinge on approximations that are either computationally expensive or lack strong theoretical guarantees. In this work, we introduce an efficient diffusion sampling algorithm for linear inverse problems that is guaranteed to be asymptotically accurate. We reveal a link between Bayesian posterior sampling and Bayesian filtering in diffusion models, proving the former as a specific instance of the latter. Our method, termed filtering posterior sampling, leverages sequential Monte Carlo methods to solve the corresponding filtering problem. It seamlessly integrates with all Markovian diffusion samplers, requires no model re-training, and guarantees accurate samples from the Bayesian posterior as particle counts rise. Empirical tests demonstrate that our method generates better or comparable results than leading zero-shot diffusion posterior samplers on tasks like image inpainting, super-resolution, and deblurring.},
  pdf={https://openreview.net/pdf?id=tplXNcHZs1},
  year={2024},  
  code={https://github.com/ZehaoDou-official/FPS-SMC-2023},
  booktitle={the 12th International Conference on Learning Representations, 2024.}
}

@inproceedings{song2023consistency,
  title={Consistency Models},
  author={Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
  booktitle={the 40th International Conference on Machine Learning, 2023.},
  abbr={ICML},
  abstract={Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like CIFAR-10, ImageNet 64x64 and LSUN 256x256.},
  pdf={https://arxiv.org/abs/2303.01469},
  code={https://github.com/openai/consistency_models},
  media={https://techcrunch.com/2023/04/12/openai-looks-beyond-diffusion-with-consistency-based-image-generator/},
  year={2023},
  poster={ICML2023/consistency.pdf},
  selected={true}
}

@article{yang2022diffusion,
  title={Diffusion Models: A Comprehensive Survey of Methods and Applications},
  author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  journal={ACM Computing Surveys},
  abbr={ACM},
  abstract={Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration.},
  code={https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
  pdf={https://arxiv.org/abs/2209.00796},
  year={2022}
}

@phdthesis{song2022learning,
  title={Learning to Generate Data by Estimating Gradients of the Data Distribution},
  author={Yang Song},
  year={2022},
  school={Stanford University},
  abbr={Thesis},
  abstract={Generating realistic data with complex patterns, such as images, audio, or molecular structures, often relies on expressive probabilistic models to represent and estimate high- dimensional data distributions. However, even with the power of deep neural networks, building powerful probabilistic models is non-trivial. One major challenge is the need to normalize probability distributions; that is, to ensure the total probability equals one. This necessitates summing over all possible model outputs, which quickly becomes impractical in high-dimensional spaces. In this dissertation, I propose to address this difficulty by working with data distributions through their score functions. These functions, defined as gradients of log data densities, capture information about the corresponding data distributions without requiring normalization, hence can be modeled with highly flexible deep neural networks. This dissertation is organized into three parts. In Part I, I show how to estimate the score function from a finite dataset with expressive deep neural networks and efficient statistical methods. In Part II, I discuss several ways to generate new data samples from models of score functions, building upon ideas from homotopy methods, Markov chain Monte Carlo, diffusion processes, and differential equations. The resulting score-based generative models (also known as diffusion models) achieved record-breaking generation performance for numerous data modalities, challenging the long-standing dominance of generative adversarial networks on many tasks. Importantly, the sampling procedure of score-based generative models can be flexibly controlled for solving inverse problems, demonstrated by their superior performance on multiple tasks in medical image reconstruction. In Part III, I show how to evaluate probability values accurately with models of score functions. Taken together, score-based generative models provide a flexible, powerful and versatile solution for data generation in machine learning and many other disciplines of science and engineering.},
  pdf={https://searchworks.stanford.edu/view/14310542},
  selected={true}
}

@inproceedings{song2022solving,
	title={Solving Inverse Problems in Medical Imaging with Score-Based Generative Models
	},
	author={Yang Song* and Liyue Shen* and Lei Xing and Stefano Ermon},
	booktitle={the 10th International Conference on Learning Representations, 2022. Abridged in the NeurIPS 2021 Workshop on Deep Learning and Inverse Problems.},
	year={2022},
	abstract={
	Reconstructing medical images from partial measurements is an important inverse problem in Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). Existing solutions based on machine learning typically train a model to directly map measurements to medical images, leveraging a training dataset of paired images and measurements. These measurements are typically synthesized from images using a fixed physical model of the measurement process, which hinders the generalization capability of models to unknown measurement processes. To address this issue, we propose a fully unsupervised technique for inverse problem solving, leveraging the recently introduced score-based generative models. Specifically, we first train a score-based generative model on medical images to capture their prior distribution. Given measurements and a physical model of the measurement process at test time, we introduce a sampling method to reconstruct an image consistent with both the prior and the observed measurements. Our method does not assume a fixed measurement process during training, and can thus be flexibly adapted to different measurement processes at test time. Empirically, we observe comparable or better performance to supervised learning techniques in several medical imaging tasks in CT and MRI, while demonstrating significantly better generalization to unknown measurement processes.
	},
	abbr={ICLR},
	pdf={https://arxiv.org/abs/2111.08005},
	code={https://github.com/yang-song/score_inverse_problems},
	selected={true}
}



@inproceedings{meng2022sdedit,
	title={SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
	author={Chenlin Meng and Yutong He and Yang Song and Jiaming Song and Jiajun Wu and Jun-Yan Zhu and Stefano Ermon},
	booktitle={the 10th International Conference on Learning Representations, 2022.},
	year={2022},
	abstract={We introduce a new image editing and synthesis framework, Stochastic Differential Editing (SDEdit), based on a recent generative model using stochastic differential equations (SDEs). Given an input image with user edits (e.g., hand-drawn color strokes), we first add noise to the input according to an SDE, and subsequently denoise it by simulating the reverse SDE to gradually increase its likelihood under the prior. Our method does not require task-specific loss function designs, which are critical components for recent image editing methods based on GAN inversion. Compared to conditional GANs, we do not need to collect new datasets of original and edited images for new applications. Therefore, our method can quickly adapt to various editing tasks at test time without re-training models. Our approach achieves strong performance on a wide range of applications, including image synthesis and editing guided by stroke paintings and image compositing.},
	abbr={ICLR},
	pdf={https://arxiv.org/abs/2108.01073},
	code={https://github.com/ermongroup/SDEdit},
	website={https://chenlin9.github.io/SDEdit/},
	media={https://www.louisbouchard.ai/image-synthesis-from-sketches/}
}

@inproceedings{xu2022geodiff,
	title={GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation
	},
	author={Minkai Xu and Lantao Yu and Yang Song and Chence Shi and Stefano Ermon and Jian Tang},
	booktitle={the 10th International Conference on Learning Representations, 2022.},
	year={2022},
	abstract={
	Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning approaches, especially with deep generative models. Inspired by the diffusion process in classical non-equilibrium thermodynamics where heated particles will diffuse from original states to a noise distribution, in this paper, we propose a novel generative model named GeoDiff for molecular conformation prediction. GeoDiff treats each atom as a particle and learns to directly reverse the diffusion process (i.e., transforming from a noise distribution to stable conformations) as a Markov chain. Modeling such a generation process is however very challenging as the likelihood of conformations should be roto-translational invariant. We theoretically show that Markov chains evolving with equivariant Markov kernels can induce an invariant distribution by design, and further propose building blocks for the Markov kernels to preserve the desirable equivariance property. The whole framework can be efficiently trained in an end-to-end fashion by optimizing a weighted variational lower bound to the (conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff is superior or comparable to existing state-of-the-art approaches, especially on large molecules. 
	},
	abbr={ICLR},
	award={Oral},
  code={https://github.com/MinkaiXu/GeoDiff},
	honor={Oral Presentation [Top 1.6\%]},
	pdf={https://openreview.net/forum?id=PzcvxEMzvQC}	
}


@inproceedings{kristy2022density,
	title={Density Ratio Estimation via Infinitesimal Classification},
	author={Kristy Choi and Chenlin Meng and Yang Song and Stefano Ermon},
	booktitle={the 25th International Conference on Artificial Intelligence and Statistics, 2022.},
	year={2022},
	abstract={
	Density ratio estimation (DRE) is a fundamental machine learning technique for comparing two probability distributions. However, existing methods struggle in high-dimensional settings, as it is difficult to accurately compare probability distributions based on finite samples. In this work we propose DRE-\infty, a divide-and-conquer approach to reduce DRE to a series of easier subproblems. Inspired by Monte Carlo methods, we smoothly interpolate between the two distributions via an infinite continuum of intermediate bridge distributions. We then estimate the instantaneous rate of change of the bridge distributions indexed by time (the "time score") -- a quantity defined analogously to data (Stein) scores -- with a novel time score matching objective. Crucially, the learned time scores can then be integrated to compute the desired density ratio. In addition, we show that traditional (Stein) scores can be used to obtain integration paths that connect regions of high density in both distributions, improving performance in practice. Empirically, we demonstrate that our approach performs well on downstream tasks such as mutual information estimation and energy-based modeling on complex, high-dimensional datasets.
	},
	abbr={AISTATS},
	award={Oral},
  code={https://github.com/ermongroup/dre-infinity},
	honor={Oral Presentation [Top 2.6%]},
	pdf={https://arxiv.org/abs/2111.11010}
}



@inproceedings{zimmermann2021score,
	title={Score-Based Generative Classifiers},
	author={Roland S. Zimmermann and Lukas Schott and Yang Song and Benjamin A. Dunn and David A. Klindt},
	booktitle={arXiv preprint arXiv:2110.00473. Abridged in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.},
	year={2021},
	abstract={
	The tremendous success of generative models in recent years raises the question whether they can also be used to perform classification. Generative models have been used as adversarially robust classifiers on simple datasets such as MNIST, but this robustness has not been observed on more complex datasets like CIFAR-10. Additionally, on natural image datasets, previous results have suggested a trade-off between the likelihood of the data and classification accuracy. In this work, we investigate score-based generative models as classifiers for natural images. We show that these models not only obtain competitive likelihood values but simultaneously achieve state-of-the-art classification accuracy for generative classifiers on CIFAR-10. Nevertheless, we find that these models are only slightly, if at all, more robust than discriminative baseline models on out-of-distribution tasks based on common image corruptions. Similarly and contrary to prior results, we find that score-based are prone to worst-case distribution shifts in the form of adversarial perturbations. Our work highlights that score-based generative models are closing the gap in classification accuracy compared to standard discriminative models. While they do not yet deliver on the promise of adversarial and out-of-domain robustness, they provide a different approach to classification that warrants further research.
	},
	abbr={arXiv},
	pdf={https://arxiv.org/abs/2110.00473}
}

@inproceedings{song2021mle,
	title={Maximum Likelihood Training of Score-Based Diffusion Models},
	author={Yang Song* and Conor Durkan* and Iain Murray and Stefano Ermon},
	booktitle={the 35th Conference on Neural Information Processing Systems, 2021.},
	year={2021},
	abstract={
	Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32x32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.
	},
	abbr={NeurIPS},
	honor={Spotlight Presentation [top 3%]},
	award={Spotlight},
	poster={NeurIPS2021/score_flow.pdf},
	selected={true},
	pdf={https://arxiv.org/abs/2101.09258},
	code={https://github.com/yang-song/score_flow}
}


@inproceedings{meng2021estimating,
	title={Estimating High Order Gradients of the Data Distribution by Denoising},
	author={Chenlin Meng and Yang Song and Wenzhe Li and Stefano Ermon},
	booktitle={the 35th Conference on Neural Information Processing Systems, 2021.},
	year={2021},
	abstract={
	The first order derivative of a data density can be estimated efficiently by denoising
	score matching, and has become an important component in many applications,
	such as image generation and audio synthesis. Higher order derivatives provide
	additional local information about the data distribution and enable new applications.
	Although they can be estimated via automatic differentiation of a learned density
	model, this can amplify estimation errors and is expensive in high dimensional
	settings. To overcome these limitations, we propose a method to directly estimate
	high order derivatives (scores) of a data density from samples. We first show
	that denoising score matching can be interpreted as a particular case of Tweedie’s
	formula. By leveraging Tweedie’s formula on higher order moments, we generalize
	denoising score matching to estimate higher order derivatives. We demonstrate
	empirically that models trained with the proposed method can approximate second
	order derivatives more efficiently and accurately than via automatic differentiation.
	We show that our models can be used to quantify uncertainty in denoising and
	to improve the mixing speed of Langevin dynamics via Ozaki discretization for
	sampling synthetic data and natural images.
	},
	abbr={NeurIPS},
	pdf={https://arxiv.org/abs/2111.04726},
	code={https://github.com/chenlin9/high_order_dsm}
}

@inproceedings{yu2021pseudo,
	title={Pseudo-Spherical Contrastive Divergence},
	author={Lantao Yu and Jiaming Song and Yang Song and Stefano Ermon},
	booktitle={the 35th Conference on Neural Information Processing Systems, 2021.},
	year={2021},
	abstract={Energy-based models (EBMs) offer flexible distribution parametrization. However, due to the intractable partition function, they are typically trained via contrastive divergence for maximum likelihood estimation. In this paper, we propose pseudo-spherical contrastive divergence (PS-CD) to generalize maximum likelihood learning of EBMs. PS-CD is derived from the maximization of a family of strictly proper homogeneous scoring rules, which avoids the computation of the intractable partition function and provides a generalized family of learning objectives that include contrastive divergence as a special case. Moreover, PS-CD allows us to flexibly choose various learning objectives to train EBMs without additional  computational cost or variational minimax optimization. Theoretical analysis on the proposed method and experiments on both synthetic data and commonly used image datasets demonstrate the effectiveness of PS-CD and its superiority over maximum likelihood and -EBMs. Based on a set of recently proposed indicative generative model evaluation metrics, we also provide an analysis on the modeling tradeoffs of different objectives in the PS-CD family on image generation tasks, justifying its modeling flexibility.
	},
	abbr={NeurIPS},
	pdf={https://openreview.net/pdf?id=8qa6hkGYDJk}
}



@inproceedings{kim2020imitation,
	title={Imitation with Neural Density Models},
	author={Kuno Kim and Akshat Jindal and Yang Song and Jiaming Song and Yanan Sui and Stefano Ermon},
	booktitle={the 35th Conference on Neural Information Processing Systems, 2021.},
	year={2021},
	abstract={We propose a new framework for Imitation Learning (IL) via density estimation of the expert's occupancy measure followed by Maximum Occupancy Entropy Reinforcement Learning (RL) using the density as a reward. Our approach maximizes a non-adversarial model-free RL objective that provably lower bounds reverse Kullback-Leibler divergence between occupancy measures of the expert and imitator. We present a practical IL algorithm, Neural Density Imitation (NDI), which obtains state-of-the-art demonstration efficiency on benchmark control tasks.
	},
	abbr={NeurIPS},
	pdf={https://arxiv.org/abs/2010.09808},
}


@inproceedings{tashiro2021csdi,
	title={CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation},
	author={Yusuke Tashiro and Jiaming Song and Yang Song and Stefano Ermon},
	booktitle={the 35th Conference on Neural Information Processing Systems, 2021.},
	year={2021},
	abstract={
	The imputation of missing values in time series has many applications in healthcare and finance. While autoregressive models are natural candidates for time series imputation, score-based diffusion models have recently outperformed existing counterparts including autoregressive models in many tasks such as image generation and audio synthesis, and would be promising for time series imputation. In this paper, we propose Conditional Score-based Diffusion models for Imputation (CSDI), a novel time series imputation method that utilizes score-based diffusion models conditioned on observed data. Unlike existing score-based approaches, the conditional diffusion model is explicitly trained for imputation and can exploit correlations between observed values. On healthcare and environmental data, CSDI improves by 40-70% over existing probabilistic imputation methods on popular performance metrics. In addition, deterministic imputation by CSDI reduces the error by 5-20% compared to the state-of-the-art deterministic imputation methods. Furthermore, CSDI can also be applied to time series interpolation and probabilistic forecasting, and is competitive with existing baselines.
	},
	abbr={NeurIPS},
	pdf={https://arxiv.org/abs/2107.03502},
	code={https://github.com/ermongroup/csdi}
}



@inproceedings{song2021nonlinear,
	title={Accelerating Feedforward Computation via Parallel Nonlinear Equation Solving},
	author={Yang Song and Chenlin Meng and Renjie Liao and Stefano Ermon},
	booktitle={the 38th International Conference on Machine Learning, 2021.},
	year={2021},
	abstract={
	Feedforward computation, such as evaluating a neural network or sampling from an autoregressive model, is ubiquitous in machine learning. The sequential nature of feedforward computation, however, requires a strict order of execution and cannot be easily accelerated with parallel computing. To enable parallelization, we frame the task of feedforward computation as solving a system of nonlinear equations. We then propose to find the solution using a Jacobi or Gauss-Seidel fixed-point iteration method, as well as hybrid methods of both. Crucially, Jacobi updates operate independently on each equation and can be executed in parallel. Our method is guaranteed to give exactly the same values as the original feedforward computation with a reduced (or equal) number of parallel iterations, and hence reduced time given sufficient parallel computing power. Experimentally, we demonstrate the effectiveness of our approach in accelerating (i) backpropagation of RNNs; (ii) evaluation of DenseNets; and (iii) autoregressive sampling of MADE and PixelCNN++, with speedup factors between 1.12 and 33 under various settings.
	},
	abbr={ICML},
	selected={true},
	pdf={https://arxiv.org/abs/2002.03629},
	code={https://github.com/ermongroup/fast_feedforward_computation},
	poster={ICML2021/fast_sampling.pdf}
}



@inproceedings{song2021how,
	title={How to Train Your Energy-Based Models},
	author={Yang Song and Diederik P. Kingma},
	booktitle={``Probabilistic Machine Learning: Advanced Topics'' by Kevin P. Murphy.},
	year={2021},
	abstract={
	Energy-Based Models (EBMs), also known as non-normalized probabilistic models, specify probability density or mass functions up to an unknown normalizing constant. Unlike most other probabilistic models, EBMs do not place a restriction on the tractability of the normalizing constant, thus are more flexible to parameterize and can model a more expressive family of probability distributions. However, the unknown normalizing constant of EBMs makes training particularly difficult. Our goal is to provide a friendly introduction to modern approaches for EBM training. We start by explaining maximum likelihood training with Markov chain Monte Carlo (MCMC), and proceed to elaborate on MCMC-free approaches, including Score Matching (SM) and Noise Constrastive Estimation (NCE). We highlight theoretical connections among these three approaches, and end with a brief survey on alternative training methods, which are still under active research. Our tutorial is targeted at an audience with basic understanding of generative models who want to apply EBMs or start a research project in this direction.
	},
	abbr={Book Chapter},
	pdf={https://arxiv.org/abs/2101.03288},
	media={https://xianblog.wordpress.com/2021/04/07/training-energy-based-models/}
}

@inproceedings{song2021score,
	title={Score-Based Generative Modeling through Stochastic Differential Equations},
	author={Yang Song and Jascha Sohl-Dickstein and Diederik P. Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
	booktitle={the 9th International Conference on Learning Representations, 2021.},
	year={2021},
	abstract={
	Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. 
	Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.
	},
	abbr={ICLR},
	pdf={https://arxiv.org/abs/2011.13456},
	code={https://github.com/yang-song/score_sde},
	award={Oral},
	award2={Award},
	honor={Outstanding Paper Award},
	blog={https://yang-song.github.io/blog/2021/score/},
	poster={ICLR2021/score_sde.pdf},
	selected={true},
	media={https://thegradientpub.substack.com/p/update-2-killer-robots-and-diffusion}
}

@inproceedings{meng2021improved,
	title={Improved Autoregressive Modeling with Distribution Smoothing},
	author={Chenlin Meng and Jiaming Song and Yang Song and Shengjia Zhao and Stefano Ermon},
	booktitle={the 9th International Conference on Learning Representations, 2021.},
	year={2021},
	abstract={
	While autoregressive models excel at image compression, their sample quality is often lacking. Inspired by randomized smoothing for adversarial defense, we incorporate randomized smoothing techniques into autoregressive generative modeling. We first model a smoothed version of the data distribution and then recover the data distribution by learning to reverse the smoothing process. We demonstrate empirically on a 1-d dataset that by appropriately choosing the smoothing level, we can keep the proposed process relatively easier to model than directly learning a data distribution with a high Lipschitz constant. Since autoregressive generative modeling consists of a sequence of 1-d density estimation problems, we believe the same arguments can be generalized to an autoregressive model. This seemingly simple procedure drastically improves the sample quality of existing autoregressive models on several synthetic and real-world datasets while obtaining competitive likelihoods on synthetic datasets.
	},
	abbr={ICLR},
	pdf={https://openreview.net/pdf?id=rJA5Pz7lHKb},
	award={Oral},
	honor={Oral Presentation [top 1.8\%]},
	code={https://github.com/chenlin9/Autoregressive-Modeling-with-Distribution-Smoothing}
}
@inproceedings{gao2020learning,
	title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
	author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P. Kingma},
	booktitle={the 9th International Conference on Learning Representations, 2021.},
	year={2021},
	abstract={
	While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained by maximizing the recovery likelihood: the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. The recovery likelihood objective is more tractable than the marginal likelihood objective, since it only requires MCMC sampling from a relatively concentrated conditional distribution. Moreover, we show that this estimation method is theoretically consistent: it learns the correct conditional and marginal distributions at each noise level, given sufficient data. After training, synthesized images can be generated efficiently by a sampling process that initializes from a spherical Gaussian distribution and progressively samples the conditional distributions at decreasingly lower noise levels. Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.60 and inception score 8.58, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets.
	},
	abbr={ICLR},
	pdf={https://arxiv.org/abs/2012.08125},
	code={https://github.com/ruiqigao/recovery_likelihood}
}


@inproceedings{gao2020learning,
	title={Anytime Sampling for Autoregressive Models via Ordered Autoencoding},
	author={Yilun Xu and Yang Song and Sahaj Garg and Linyuan Gong and Rui Shu and Aditya Grover and Stefano Ermon},
	booktitle={the 9th International Conference on Learning Representations, 2021.},
	year={2021},
	abstract={
		Autoregressive models are widely used for tasks such as image and audio generation. The sampling process of these models, however, does not allow interruptions and cannot adapt to real-time computational resources. This challenge impedes the deployment of powerful autoregressive models, which involve a slow sampling process that is sequential in nature and typically scales linearly with respect to the data dimension.  To address this difficulty, we propose a new family of autoregressive models that enables anytime sampling. Inspired by Principal Component Analysis, we learn a structured representation space where dimensions are ordered based on their importance with respect to reconstruction. Using an autoregressive model in this latent space, we trade off sample quality for computational efficiency by truncating the generation process before decoding into the original data space. Experimentally, we demonstrate in several image and audio generation tasks that sample quality degrades gracefully as we reduce the computational budget for sampling. The approach suffers almost no loss in sample quality (measured by FID) using only 60\% to 80\% of all latent dimensions for image data.
	},
	abbr={ICLR},
	pdf={https://openreview.net/pdf?id=TSRTzJnuEBS},
	code={https://github.com/Newbeeer/Anytime-Auto-Regressive-Model}
}

@inproceedings{song2020improved,
  title={Improved Techniques for Training Score-Based Generative Models},
  author={Yang Song and Stefano Ermon},
  booktitle={the 34th Conference on Neural Information Processing Systems, 2020.},
  year={2020},
  abstract={
    Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.
  },
  pdf={https://arxiv.org/abs/2006.09011},
  code={https://github.com/ermongroup/ncsnv2},
  abbr={NeurIPS},
  blog={https://yang-song.github.io/blog/2021/score/},
  poster={NeurIPS2020/ncsnv2-poster.pdf},
  selected={true},
}


@inproceedings{tashiro2020output,
  title={Diversity can be Transferred: Output Diversification for White- and Black-box Attacks},
  author={Yusuke Tashiro and Yang Song and Stefano Ermon},
  booktitle={the 34th Conference on Neural Information Processing Systems, 2020.},
  year={2020},
  abstract={
    Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, e.g., to initialize optimization-based white-box attacks or generate update directions in black-box attacks. These simple perturbations, however, could be suboptimal as they are agnostic to the model being attacked. To improve the efficiency of these attacks, we propose Output Diversified Sampling (ODS), a novel sampling strategy that attempts to maximize diversity in the target model's outputs among the generated samples. While ODS is a gradient-based strategy, the diversity offered by ODS is transferable and can be helpful for both white-box and black-box attacks via surrogate models. Empirically, we demonstrate that ODS significantly improves the performance of existing white-box and black-box attacks. In particular, ODS reduces the number of queries needed for state-of-the-art black-box attacks on ImageNet by a factor of two.
  },
  pdf={https://arxiv.org/abs/2003.06878},
  code={https://github.com/ermongroup/ODI/},
  abbr={NeurIPS},
}


@inproceedings{meng2020autoregressive,
	title={Autoregressive Score Matching},
	author={Chenlin Meng and Lantao Yu and Yang Song and Jiaming Song and Stefano Ermon},
	booktitle={the 34th Conference on Neural Information Processing Systems, 2020.},
	year={2020},
	abstract={
	Autoregressive models use the chain rule to define a joint probability distribution as a product of conditionals. These conditionals need to be normalized, imposing constraints on the functional families that can be used. To increase flexibility, we propose autoregressive conditional score models (AR-CSM) and parameterize the joint distribution in terms of the derivatives of univariate log-conditionals (scores), which need not be normalized. To train AR-CSM, we introduce a new divergence between distributions named Composite Score Matching (CSM). For AR-CSM models, this divergence between data and model distributions can be computed and optimized efficiently, requiring no expensive sampling or adversarial training. Compared to previous score matching algorithms, our method is more scalable to high dimensional data and more stable to optimize. We show with extensive experimental results that it can be applied to density estimation on synthetic data, image generation, image denoising, and training latent variable models with implicit encoders.
	},
	abbr={NeurIPS},
	pdf={https://arxiv.org/abs/2010.12810},
}


@inproceedings{pang2020efficient,
	title={Efficient Learning of Generative Models via Finite-Difference Score Matching},
	author={Pang, Tianyu and Xu, Kun and Li, Chongxuan and Song, Yang and Ermon, Stefano and Zhu, Jun},
	booktitle={the 34th Conference on Neural Information Processing Systems, 2020.},
	abstract={Several machine learning applications involve the optimization of higher-order derivatives (e.g., gradients of gradients) during training, which can be expensive in respect to memory and computation even with automatic differentiation. As a typical example in generative modeling, score matching (SM) involves the optimization of the trace of a Hessian. To improve computing efficiency, we rewrite the SM objective and its variants in terms of directional derivatives, and present a generic strategy to efficiently approximate any-order directional derivative with finite difference (FD). Our approximation only involves function evaluations, which can be executed in parallel, and no gradient computations. Thus, it reduces the total computational cost while also improving numerical stability. We provide two instantiations by reformulating variants of SM objectives into the FD forms. Empirically, we demonstrate that our methods produce results comparable to the gradient-based counterparts while being much more computationally efficient.},
	year={2020},
	pdf={https://arxiv.org/abs/2007.03317},
	code={https://github.com/taufikxu/FD-ScoreMatching},
	abbr={NeurIPS},
}




@inproceedings{yu2020training,
  title={Training Deep Energy-Based Models with f-Divergence Minimization},
  author={Lantao Yu and Yang Song and Jiaming Song and Stefano Ermon},
  booktitle={the 37th International Conference on Machine Learning, 2020.},
  year={2020},
  abbr={ICML},
  abstract={
    Deep energy-based models (EBMs) are very flexible in distribution parametrization but computationally challenging because of the intractable partition function. They are typically trained via maximum likelihood, using contrastive divergence to approximate the gradient of the KL divergence between data and model distribution. While KL divergence has many desirable properties, other f-divergences have shown advantages in training implicit density generative models such as generative adversarial networks. In this paper, we propose a general variational framework termed f-EBM to train EBMs using any desired f-divergence. We introduce a corresponding optimization algorithm and prove its local convergence property with non-linear dynamical systems theory. Experimental results demonstrate the superiority of f-EBM over contrastive divergence, as well as the benefits of training EBMs using f-divergences other than KL.
  },
  pdf={https://arxiv.org/pdf/2003.03463.pdf},
  code={https://github.com/ermongroup/f-EBM}
}

@inproceedings{meng2020gaussianization,
  title={Gaussianization Flows},
  author={Chenlin Meng* and Yang Song* and Jiaming Song and Stefano Ermon},
  booktitle={the 23rd International Conference on Artificial Intelligence and Statistics, 2020.},
  year={2020},
  abbr={AISTATS},
  abstract={
    Iterative Gaussianization is a fixed-point iteration procedure that allows one to transform a continuous distribution to Gaussian distribution. Based on iterative Gaussianization, we propose a new type of normalizing flow models that grants both efficient computation of likelihoods and efficient inversion for sample generation. We demonstrate that this new family of flow models, named as Gaussianization flows, are universal approximators for continuous probability distributions under some regularity conditions. This guaranteed expressivity, enabling them to capture multimodal target distributions better without compromising the efficiency in sample generation. Experimentally, we show that Gaussianization flows achieve better or comparable performance on several tabular datasets, compared to other efficiently invertible flow models such as Real NVP, Glow and FFJORD. In particular, Gaussianization flows are easier to initialize, demonstrate better robustness with respect to different transformations of the training data, and generalize better on small training settings
  },
  pdf={https://arxiv.org/abs/2003.01941},
  code={https://github.com/chenlin9/Gaussianization_Flows},
}

@inproceedings{niu2020permutation,
  title={Permutation Invariant Graph Generation via Score-Based Generative Modeling},
  author={Chenhao Niu and Yang Song and Jiaming Song and Shengjia Zhao and Aditya Grover and Stefano Ermon},
  booktitle={the 23rd International Conference on Artificial Intelligence and Statistics, 2020.},
  year={2020},
  abbr={AISTATS},
  abstract={
    Learning generative models for graph structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.
  },
  pdf={https://arxiv.org/abs/2003.00638},
  code={https://github.com/ermongroup/GraphScoreMatching},
}

@inproceedings{song2019unsupervised,
  title={Unsupervised Out-of-Distribution Detection with Batch Normalization},
  author={Jiaming Song and Yang Song and Stefano Ermon},
  booktitle={Technical report (10/21/2019).},
  year={2019},
  abstract={
    Likelihood from a generative model is a natural statistic for detecting out-of-distribution (OoD) samples. However, generative models have been shown to assign higher likelihood to OoD samples compared to ones from the training distribution, preventing simple threshold-based detection rules. We demonstrate that OoD detection fails even when using more sophisticated statistics based on the likelihoods of individual samples. To address these issues, we propose a new method that leverages batch normalization. We argue that batch normalization for generative models challenges the traditional i.i.d. data assumption and changes the corresponding maximum likelihood objective. Based on this insight, we propose to exploit in-batch dependencies for OoD detection. Empirical results suggest that this leads to more robust detection for high-dimensional images.
  },
  abbr={arXiv},
  pdf={https://arxiv.org/abs/1910.09115}
}


@inproceedings{zhao2019towards,
	title={Towards Certified Defense for Unrestricted Adversarial Attacks },
	author={Shengjia Zhao and Yang Song and Stefano Ermon},
	booktitle={Technical report, 2019.},
	year={2019},
	abstract={
	Certified defenses against adversarial examples are very important in safety-critical applications of machine learning. However, existing certified defense strategies only safeguard against perturbation-based adversarial attacks, where the attacker is only allowed to modify normal data points by adding small perturbations. In this paper, we provide certified defenses under the more general threat model of unrestricted adversarial attacks. We allow the attacker to generate arbitrary inputs to fool the classifier, and assume the attacker knows everything except the classifiers' parameters and the training dataset used to learn it. Lack of knowledge about the classifiers parameters prevents an attacker from generating adversarial examples successfully. Our defense draws inspiration from differential privacy, and is based on intentionally adding noise to the classifier's outputs to limit the attacker's knowledge about the parameters. We prove concrete bounds on the minimum number of queries required for any attacker to generate a successful adversarial attack. For a simple linear classifiers we prove that the bound is asymptotically optimal up to a constant by exhibiting an attack algorithm that achieves this lower bound. We empirically show the success of our defense strategy against strong black box attack algorithms.
	},
	abbr={OpenReview},
	pdf={https://openreview.net/pdf?id=S1lBVgHYvr}
}


@inproceedings{song2019generative,
  title={Generative Modeling by Estimating Gradients of the Data Distribution},
  author={Song, Yang and Ermon, Stefano},
  booktitle={the 33rd Conference on Neural Information Processing Systems, 2019.},
  year={2019},
  abbr={NeurIPS},
  award={Oral},
  honor={Oral Presentation [top 0.5\%]},
  abstract={
  We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients might be ill-defined when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.
  },
  pdf={https://arxiv.org/abs/1907.05600},
  code={https://github.com/ermongroup/ncsn},
  media={https://www.youtube.com/watch?v=Oc3X_x1Q1jU},
  poster={NeurIPS2019/ncsn-poster.pdf},
  slides={NeurIPS2019/talk_public.pptx},
  blog={https://yang-song.github.io/blog/2021/score/},
  selected={true}
}


@inproceedings{song2019mintnet,
  abstract={
    We propose a new way of constructing invertible neural networks by combining simple building blocks with a novel set of composition rules. This leads to a rich set of invertible architectures, including those similar to ResNets. Inversion is achieved with a locally convergent iterative procedure that is parallelizable and very fast in practice. Additionally, the determinant of the Jacobian can be computed analytically and efficiently, enabling their generative use as flow models. To demonstrate their flexibility, we show that our invertible neural networks are competitive with ResNets on MNIST and CIFAR-10 classification. When trained as generative models, our invertible networks achieve new state-of-the-art likelihoods on MNIST, CIFAR-10 and ImageNet 32x32, with bits per dimension of 0.98, 3.32 and 4.06 respectively.
  },
  title={
MintNet: Building Invertible Neural Networks with Masked Convolutions    
  },
  author = {Yang Song* and Chenlin Meng* and Stefano Ermon},
  booktitle = {the 33rd Conference on Neural Information Processing Systems, 2019.},
  year={2019},
  abbr={NeurIPS},
  pdf={https://arxiv.org/abs/1907.07945},
  code={https://github.com/ermongroup/mintnet},
  poster={NeurIPS/mintnet-poster.pdf}
}


@inproceedings{liao2019efficient,
  abstract={
    We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our framework better captures the auto-regressive conditioning between the already generated and to be generated parts of the graph using Graph Neural Networks (GNNs) with attention mechanisms. It not only reduces the dependency on node ordering but also bypasses the unnecessary long-term bottleneck brought by the sequential ordering of RNNs. Moreover, we parameterize the output distribution per generation step using a mixture of Bernoulli, which can model correlations among generated edges. Finally, we approximately marginalize over permutations via variational inference with a family of canonical node orderings. Our model generates graphs one block of nodes and associated edges at a time, independently conditioned on the context. The block size and sampling stride allow us to trade-off sample quality for efficiency. On standard benchmarks, we achieve the state-of-the-art performance and better sample quality compared to previous models. We also obtain very impressive results on a large graph dataset (up to 5K nodes). To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.
  },
  title={
    Efficient Graph Generation with Graph Recurrent Attention Networks
  },
  author = {Renjie Liao and Yujia Li and Yang Song and Shenlong Wang and William L. Hamilton and David Duvenaud and Raquel Urtasun and Richard Zemel},
  booktitle = {the 33rd Conference on Neural Information Processing Systems, 2019.},
  year={2019},
  abbr={NeurIPS},
  pdf={https://arxiv.org/abs/1910.00760},
  code={https://github.com/lrjconan/GRAN}
}

@inproceedings{song2019sliced,
  abstract = {
    Score matching is a popular method for estimating unnormalized statistical models. However, it has been so far limited to simple models or low-dimensional data, due to the difficulty of computing the trace of Hessians for log-density functions. We show this difficulty can be mitigated by sliced score matching, a new objective that matches random projections of the original scores. Our objective only involves Hessian-vector products, which can be easily implemented using reverse-mode auto-differentiation. This enables scalable score matching for complex models and higher dimensional data. Theoretically, we prove the consistency and asymptotic normality of sliced score matching. Moreover, we demonstrate that sliced score matching can be used to learn deep score estimators for implicit distributions. In our experiments, we show that sliced score matching greatly outperforms competitors on learning deep energy-based models, and can produce accurate score estimates for applications such as variational inference with implicit distributions and training Wasserstein Auto-Encoders.
  },
  author    = {Yang Song* and
               Sahaj Garg* and
               Jiaxin Shi and
               Stefano Ermon},
  title     = {Sliced Score Matching: {A} Scalable Approach to Density and Score
               Estimation},
  booktitle = {the 35th Conference on Uncertainty in Artificial Intelligence, 2019.},
  pages     = {204},
  year      = {2019},
  award = {Oral},
  pdf = {https://arxiv.org/abs/1905.07088},
  abbr = {UAI},
  code = {https://github.com/ermongroup/sliced_score_matching},
  honor = {Oral Presentation [top 8.7\%]},
  blog = {https://ermongroup.github.io/blog/ssm/},
  selected={true},
}

@inproceedings{song2018construct,
  abstract={
    Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.
  },
  author = {Yang Song and Rui Shu and Nate Kushman and Stefano Ermon},
  title={
    Constructing Unrestricted Adversarial Examples with Generative Models
  },
  booktitle={the 32nd Conference on Neural Information Processing Systems, 2018.},
  year={2018},
  abbr={NeurIPS},
  pdf={https://arxiv.org/abs/1805.07894},
  poster={NeurIPS2018/uae-poster.pdf},
  code={https://github.com/ermongroup/generative_adversary},
  media={https://twitter.com/goodfellow_ian/status/999035763596578816},
  selected={true}
}


@inproceedings{song2018accelerating,
  abstract={
    An appealing property of the natural gradient is that it is invariant to arbitrary differentiable reparameterizations of the model. However, this invariance property requires infinitesimal steps and is lost in practical implementations with small but finite step sizes. In this paper, we study invariance properties from a combined perspective of Riemannian geometry and numerical differential equation solving. We define the order of invariance of a numerical method to be its convergence order to an invariant solution. We propose to use higher-order integrators and geodesic corrections to obtain more invariant optimization trajectories. We prove the numerical convergence properties of geodesic corrected updates and show that they can be as computational efficient as plain natural gradient. Experimentally, we demonstrate that invariance leads to faster optimization and our techniques improve on traditional natural gradient in deep neural network training and natural policy gradient for reinforcement learning.
  },
  author = {Yang Song and Jiaming Song and Stefano Ermon},
  title = {
    Accelerating Natural Gradient with Higher-Order Invariance
  },
  booktitle={
    the 35th International Conference on Machine Learning, 2018.
  },
  year={2018},
  abbr={ICML},
  pdf={https://arxiv.org/abs/1803.01273},
  poster={ICML2018/geo-poster.pdf},
  code={https://github.com/ermongroup/higher_order_invariance},
  blog={https://ermongroup.github.io/blog/geo/}
}

@inproceedings{song2018pixeldefend,
  abstract={
    Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.
  },
  author={Yang Song and Taesup Kim and Sebastian Nowozin and Stefano Ermon and Nate Kushman},
  title={
    PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples
  },
  booktitle={
    the 6th International Conference on Learning Representations, 2018.
  },
  year={2018},
  abbr={ICLR},
  pdf={https://arxiv.org/abs/1710.10766},
  poster={ICLR2018/pixeldefend-poster.pdf},
  code={https://github.com/Microsoft/PixelDefend/},
  selected={true}
}

@inproceedings{song2016kernel,
  abstract={
    We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines.
  },
  author={Yang Song and Jun Zhu and Yong Ren},
  title={
    Kernel Bayesian Inference with Posterior Regularization
  },
  booktitle={
    the 30th Conference on Neural Information Processing Systems, 2016.
  },
  year={2016},
  abbr={NeurIPS},
  pdf={https://arxiv.org/abs/1607.02011},
  poster={NIPS2016/kRegBayes_poster.pdf}
}

@inproceedings{liu2016stochastic,
  abstract={
    We propose two stochastic gradient MCMC methods for sampling from Bayesian posterior distributions defined on Riemann manifolds with a known geodesic flow, e.g. hyperspheres. Our methods are the first scalable sampling methods on these manifolds, with the aid of stochastic gradients. Novel dynamics are conceived and second-order integrators are developed. By adopting embedding techniques and the geodesic integrator, the methods do not require a global coordinate system of the manifold and do not involve inner iterations. Synthetic experiments show the validity of the method, and its application to the challenging inference for spherical topic models indicate practical usability and efficiency.
  },
  author={Chang Liu and Jun Zhu and Yang Song},
  title={
    Stochastic Gradient Geodesic MCMC Methods
  },
  booktitle={
    the 30th Conference on Neural Information Processing Systems, 2016.
  },
  year={2016},
  abbr={NeurIPS},
  pdf={https://papers.nips.cc/paper/6282-stochastic-gradient-geodesic-mcmc-methods},
  code={http://ml.cs.tsinghua.edu.cn/\~{}changliu/sggmcmc-sam/}
}

@inproceedings{song2016training,
  abstract={
    Supervised training of deep neural nets typically relies on minimizing cross-entropy. However, in many domains, we are interested in performing well on metrics specific to the application. In this paper we propose a direct loss minimization approach to train deep neural networks, which provably minimizes the application-specific loss function. This is often non-trivial, since these functions are neither smooth nor decomposable and thus are not amenable to optimization with standard gradient-based methods. We demonstrate the effectiveness of our approach in the context of maximizing average precision for ranking problems. Towards this goal, we develop a novel dynamic programming algorithm that can efficiently compute the weight updates. Our approach proves superior to a variety of baselines in the context of action classification and object detection, especially in the presence of label noise.
  },
  author={Yang Song and Alexander Schwing and Richard Zemel and Raquel Urtasun},
  title={Training Deep Neural Networks via Direct Loss Minimization},
  booktitle={the 33rd International Conference on Machine Learning, 2016.},
  year={2016},
  abbr={ICML},
  pdf={http://arxiv.org/abs/1511.06411},
  supp={ICML2016/supplementary_materials.pdf},
  code={https://github.com/yang-song/APDLM},
  video={http://techtalks.tv/talks/training-deep-neural-networks-via-direct-loss-minimization/62578/}
}

@inproceedings{song2016bayesian,
  abstract={
    Bayesian matrix completion has been studied based on a low-rank matrix factorization formulation with promising results. However, little work has been done on Bayesian matrix completion based on the more direct spectral regularization formulation. We fill this gap by presenting a novel Bayesian matrix completion method based on spectral regularization. In order to circumvent the difficulties of dealing with the orthonormality constraints of singular vectors, we derive a new equivalent form with relaxed constraints, which then leads us to design an adaptive version of spectral regularization feasible for Bayesian inference. Our Bayesian method requires no parameter tuning and can infer the number of latent factors automatically. Experiments on synthetic and real datasets demonstrate encouraging results on rank recovery and collaborative filtering, with notably good results for very sparse matrices.
  },
  author={Yang Song and Jun Zhu},
  title={Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization},
  booktitle={the 30th AAAI Conference on Artificial Intelligence, 2016.},
  year={2016},
  abbr={AAAI},
  award={Oral},
  pdf={https://arxiv.org/abs/1512.01110},
  supp={AAAI2016/GASR-supp.pdf},
  slides={AAAI2016/gasr_slides.pdf},
  code={https://github.com/yang-song/GASR}
}


